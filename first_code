#first thing we do is datacleaning and standardization (standardization is recommended for regularization)
df_no_labels= select(df_training[1:5000,:], Not(:labels))
df_clean_const_no_labels = df_no_labels[:, std.(eachcol(df_no_labels)) .!= 0]
colum_labels= df_training.labels
stand_gene_mach= fit!(machine(Standardizer(), df_clean_const_no_labels));
df_training_stand_no_labels= MLJ.transform(stand_gene_mach,df_clean_const_no_labels)
df_training_stand= hcat(df_training_stand_no_labels, colum_labels)
df_training_stand[:,end-3:end] #to chech if labels were correctly added back 
rename!(df_training_stand,:x1 => :labels)
coerce!(df_training_stand, :labels => Multiclass);

we try regularization for lambda= 1e-3 with the standardized and cleaned data (1:4000)-> training, (4001:5000)-> validation
gene_fit_reg = machine(MultinomialClassifier(penalty = :l1, lambda = 1e-3),
                        select(df_training_stand[1:4000,:], Not(:labels)),
                        df_training_stand.labels[1:4000])|> fit!
			
function losses(machine, input, response)
    (negative_loglikelihood = sum(log_loss(predict(machine, input), response)),
     misclassification_rate = mean(predict_mode(machine, input) .!= response),
     accuracy = accuracy(predict_mode(machine, input), response),
     auc = auc(predict(machine, input), response),
	 confusion_matrix = confusion_matrix(predict_mode(machine, input), response))
end;
losses(gene_fit_reg, select(df_training_stand[1:4000,:], Not(:labels)), df_training_stand.labels[1:4000])
#result train accuracy:
negative_loglikelihood
77.3269
misclassification_rate
0.0
accuracy
1.0
auc
1.0
confusion_matrix
              ┌─────────────────────────────────────────┐
              │              Ground Truth               │
┌─────────────┼─────────────┬─────────────┬─────────────┤
│  Predicted  │     CBP     │    KAT5     │    eGFP     │
├─────────────┼─────────────┼─────────────┼─────────────┤
│     CBP     │    1483     │      0      │      0      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    KAT5     │      0      │    1270     │      0      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    eGFP     │      0      │      0      │    1247     │
└─────────────┴─────────────┴─────────────┴─────────────┘
losses(gene_fit_reg, select(df_training_stand[4001:5000,:], Not(:labels)), df_training_stand.labels[4001:5000])
#result validation accuracy:
negative_loglikelihood
290.723
misclassification_rate
0.103
accuracy
0.897
auc
0.951294
confusion_matrix
              ┌─────────────────────────────────────────┐
              │              Ground Truth               │
┌─────────────┼─────────────┬─────────────┬─────────────┤
│  Predicted  │     CBP     │    KAT5     │    eGFP     │
├─────────────┼─────────────┼─────────────┼─────────────┤
│     CBP     │     330     │     48      │      4      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    KAT5     │     37      │     231     │      5      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    eGFP     │      2      │      7      │     336     │
└─────────────┴─────────────┴─────────────┴─────────────┘


# trying PCA for the first time instead of regularization to avoid overtraining of model: We used 20 PC (output dimension= 20) and noticed immediatly that there is less overtraining 
now we're going to try CV to determine the right amount of PC's so we don't lose to many explainable variation 
gene_fit_reg = machine(PCA(maxoutdim= 20) |> MultinomialClassifier(penalty = :none),
                        #select(df_training_stand[1:4000,:], Not(:labels)),
                        #df_training_stand.labels[1:4000])|> fit!

losses(gene_fit_reg, select(df_training_stand[1:4000,:], Not(:labels)), df_training_stand.labels[1:4000])
result: 
negative_loglikelihood
1564.04
misclassification_rate
0.17925
accuracy
0.82075
auc
0.894572
confusion_matrix
              ┌─────────────────────────────────────────┐
              │              Ground Truth               │
┌─────────────┼─────────────┬─────────────┬─────────────┤
│  Predicted  │     CBP     │    KAT5     │    eGFP     │
├─────────────┼─────────────┼─────────────┼─────────────┤
│     CBP     │    1175     │     347     │      1      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    KAT5     │     303     │     894     │     32      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    eGFP     │      5      │     29      │    1214     │
└─────────────┴─────────────┴─────────────┴─────────────┘

losses(gene_fit_reg, select(df_training_stand[4001:5000,:], Not(:labels)), df_training_stand.labels[4001:5000])
negative_loglikelihood
378.894
misclassification_rate
0.181
accuracy
0.819
auc
0.896878
confusion_matrix
              ┌─────────────────────────────────────────┐
              │              Ground Truth               │
┌─────────────┼─────────────┬─────────────┬─────────────┤
│  Predicted  │     CBP     │    KAT5     │    eGFP     │
├─────────────┼─────────────┼─────────────┼─────────────┤
│     CBP     │     286     │     82      │      3      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    KAT5     │     83      │     199     │      8      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    eGFP     │      0      │      5      │     334     │
└─────────────┴─────────────┴─────────────┴─────────────┘

cross validation to find perfect amount of PC's takes to long but we got pretty similar resulats for #PC's= 190 as voor l1 regression BUT with less overtraining (see training accuracy)
gene_fit_reg = machine(PCA(maxoutdim= 190) |> MultinomialClassifier(penalty = :none),
                    	select(df_training_stand[1:4000,:], Not(:labels)),
                        df_training_stand.labels[1:4000])|> fit!
losses(gene_fit_reg, select(df_training_stand[1:4000,:], Not(:labels)), df_training_stand.labels[1:4000])
negative_loglikelihood
772.009
misclassification_rate
0.08025
accuracy
0.91975
auc
0.974035
confusion_matrix
              ┌─────────────────────────────────────────┐
              │              Ground Truth               │
┌─────────────┼─────────────┬─────────────┬─────────────┤
│  Predicted  │     CBP     │    KAT5     │    eGFP     │
├─────────────┼─────────────┼─────────────┼─────────────┤
│     CBP     │    1322     │     160     │      0      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    KAT5     │     161     │    1110     │      0      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    eGFP     │      0      │      0      │    1247     │
└─────────────┴─────────────┴─────────────┴─────────────┘

losses(gene_fit_reg, select(df_training_stand[4001:5000,:], Not(:labels)), df_training_stand.labels[4001:5000])
negative_loglikelihood
463.123
misclassification_rate
0.107
accuracy
0.893
auc
0.936941
confusion_matrix
              ┌─────────────────────────────────────────┐
              │              Ground Truth               │
┌─────────────┼─────────────┬─────────────┬─────────────┤
│  Predicted  │     CBP     │    KAT5     │    eGFP     │
├─────────────┼─────────────┼─────────────┼─────────────┤
│     CBP     │     343     │     69      │      2      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    KAT5     │     26      │     210     │      3      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    eGFP     │      0      │      7      │     340     │
└─────────────┴─────────────┴─────────────┴─────────────┘
